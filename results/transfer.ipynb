{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from time import time_ns\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from rnaquanet.network.graph_regression_network import GraphRegressionNetwork\n",
    "from rnaquanet.network.grn_data_module import GRNDataModule\n",
    "from rnaquanet.utils.rnaquanet_config import RnaquanetConfig\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "from torch.nn import (\n",
    "    BatchNorm1d,\n",
    "    Identity,\n",
    "    ReLU,\n",
    "    LeakyReLU,\n",
    "    Linear,\n",
    "    MSELoss\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    GATConv,\n",
    "    GCNConv,\n",
    "    Sequential,\n",
    "    global_mean_pool,\n",
    "    BatchNorm,\n",
    "    TransformerConv\n",
    ")\n",
    "from torch_geometric.nn.models import (\n",
    "    GAT\n",
    ")\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "from rnaquanet.data.preprocessing.hdf5_utils import load_data_from_hdf5\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from rnaquanet.network.h5_graph_dataset import H5GraphDataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ares_train_data = H5GraphDataset('/app/data/ares/train.h5').__enter__()\n",
    "\n",
    "seg1_val_data = H5GraphDataset('/app/data/segments_1_coords/val.h5').__enter__()\n",
    "seg1_test_data = H5GraphDataset('/app/data/segments_1_coords/test.h5').__enter__()\n",
    "\n",
    "seg2_val_data = H5GraphDataset('/app/data/segments_2_coords/val.h5').__enter__()\n",
    "seg2_test_data = H5GraphDataset('/app/data/segments_2_coords/test.h5').__enter__()\n",
    "\n",
    "seg3_val_data = H5GraphDataset('/app/data/segments_3_coords/val.h5').__enter__()\n",
    "seg3_test_data = H5GraphDataset('/app/data/segments_3_coords/test.h5').__enter__()\n",
    "\n",
    "ares_val_data = H5GraphDataset('/app/data/ares/val.h5').__enter__()\n",
    "ares_test_data = H5GraphDataset('/app/data/ares/test.h5').__enter__()\n",
    "\n",
    "dataset = 'transfer_seg2_ares'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_chart(losses, epoch):\n",
    "    bars = len(losses.values()) # num of bars\n",
    "    epochs = range(1, epoch+2)\n",
    "    width = 1.0/bars * 0.75\n",
    "    multiplier = 0\n",
    "    _, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "    for key, value in losses.items():\n",
    "        x = np.arange(len(value['losses']))\n",
    "\n",
    "        loss_values = value['losses']\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, loss_values, width, label=key, color=value['color'], alpha=0.5)\n",
    "        ax.bar_label(rects, fmt='%0.0f', padding=3)\n",
    "        ax.plot(x+((bars-1)/2)*width, loss_values, color=value['color'], alpha=0.5)\n",
    "        multiplier += 1\n",
    "\n",
    "    x = np.arange(len(epochs))\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_xticks(x + ((bars-1)/2)*width, epochs)\n",
    "    ax.legend()\n",
    "    plt.savefig(os.path.join('train', f'{dataset}.png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_desc(epoch, losses):\n",
    "    desc = f'Epoch {epoch+1}\\n'\n",
    "    for key, value in losses.items():\n",
    "        if value['losses']:\n",
    "            desc += f'Prev {key} {value[\"losses\"][-1]:.2f}\\tBest {key} {np.min(value[\"losses\"]):.2f}\\n'\n",
    "    return desc.rstrip('\\n')\n",
    "\n",
    "def process_epoch(\n",
    "        epoch, \n",
    "        batch_size, \n",
    "        device, \n",
    "        model,\n",
    "        loss_fn,\n",
    "        mse_no_reduce,\n",
    "        optimizer,\n",
    "        patience, \n",
    "        \n",
    "        losses, key,\n",
    "        best_model, best_loss,\n",
    "\n",
    "        save_values = False,\n",
    "        plot = True\n",
    "    ):\n",
    "    step = []\n",
    "    values = {'predicted': [], 'true': []}\n",
    "    assert(not losses[key]['train'] or not save_values)\n",
    "\n",
    "    if losses[key]['data'].max_iterations != None:\n",
    "        losses[key]['data'].shuffle()\n",
    "    \n",
    "\n",
    "    with tqdm(total=len(losses[key]['data']), desc=f'{key.capitalize()} epoch {epoch+1}', position=0, leave=True) as progressbar:\n",
    "        if not losses[key]['train']:\n",
    "            with torch.no_grad():\n",
    "                for item in DataLoader(losses[key]['data'], batch_size=batch_size, shuffle=False, num_workers=4):\n",
    "                    item = item.to(device)\n",
    "                    y_pred = model(x=item.x, edge_index=item.edge_index, edge_attr=item.edge_attr, batch=item.batch).view(-1)\n",
    "                    step.extend(mse_no_reduce(y_pred, item.y).cpu().tolist())\n",
    "                    if save_values:\n",
    "                        values['predicted'].extend(y_pred.cpu().tolist())\n",
    "                        values['true'].extend(item.y.view(-1).cpu().tolist())\n",
    "                    progressbar.update(item.y.cpu().shape[0])\n",
    "        else:\n",
    "            for item in DataLoader(losses[key]['data'], batch_size=batch_size, shuffle=(losses[key]['data'].max_iterations == None), num_workers=4):\n",
    "                item = item.to(device)\n",
    "                y_pred = model(x=item.x, edge_index=item.edge_index, edge_attr=item.edge_attr, batch=item.batch).view(-1)\n",
    "                loss = loss_fn(y_pred, item.y)\n",
    "                step.extend(mse_no_reduce(y_pred, item.y).cpu().tolist())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                progressbar.update(item.y.cpu().shape[0])\n",
    "        \n",
    "    losses[key]['losses'].append(np.mean(step))\n",
    "    if save_values:\n",
    "        losses[key]['values'].append(values)\n",
    "    clear_output(wait=True)\n",
    "    print(get_desc(epoch, losses))\n",
    "    if plot:\n",
    "        plot_loss_chart(losses, epoch)\n",
    "\n",
    "    index = np.argmin(losses[key]['losses'])\n",
    "    if losses[key]['patience']:\n",
    "        if losses[key]['losses'][-1] < best_loss:\n",
    "            return model.state_dict().copy(), losses[key]['losses'][index], False\n",
    "    return best_model, best_loss, losses[key]['patience'] and index <= (epoch-patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "torch.manual_seed(2137)\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "max_epochs = 1000\n",
    "batch_size = 32\n",
    "patience = 3\n",
    "\n",
    "model = Sequential('x, edge_index, edge_attr, batch', [\n",
    "    (TransformerConv(in_channels=99, out_channels=256, heads=4, dropout=0.5), 'x, edge_index -> x'),\n",
    "    (BatchNorm(in_channels=256*4), 'x -> x'),\n",
    "    (ReLU(), 'x -> x'),\n",
    "    (TransformerConv(in_channels=256*4, out_channels=256, heads=8, dropout=0.5), 'x, edge_index -> x'),\n",
    "    (BatchNorm(in_channels=256*8), 'x -> x'),\n",
    "    (ReLU(), 'x -> x'),\n",
    "\n",
    "    (GCNConv(in_channels=256*8, out_channels=256), 'x, edge_index -> x'),\n",
    "    (global_mean_pool, 'x, batch -> x'),\n",
    "\n",
    "    (Linear(in_features=256, out_features=64), 'x -> x'),\n",
    "    (ReLU(), 'x -> x'),\n",
    "    (Linear(in_features=64, out_features=1), 'x -> x'),\n",
    "]).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join('model', 'seg2.pt')))\n",
    "\n",
    "# freeze all layers except MLP\n",
    "for child in model.children():\n",
    "    if type(child) != Linear:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "loss_fn = MSELoss(reduction='mean').to(device)\n",
    "mse_no_reduce = MSELoss(reduction='none').to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "losses = {\n",
    "    'ares train': {\n",
    "        'data': ares_train_data,\n",
    "        'losses': [],\n",
    "        'color': 'red',\n",
    "        'train': True,\n",
    "        'patience': False\n",
    "    },\n",
    "    'ares val': {\n",
    "        'data': ares_val_data,\n",
    "        'losses': [],\n",
    "        'color': 'blue',\n",
    "        'train': False,\n",
    "        'patience': True,\n",
    "        'values': []\n",
    "    },\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    for key in losses.keys():\n",
    "        best_model, best_loss, cond = process_epoch(epoch, batch_size, device, model, loss_fn, mse_no_reduce, optimizer, patience, losses, key, best_model, best_loss)\n",
    "        if cond:\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "\n",
    "torch.save(best_model, os.path.join('model', f'{dataset}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted\tground truth\tdiff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.533008575439453\t6.302999973297119\t1.230008602142334\n",
      "8.907495498657227\t2.313999891281128\t6.593495607376099\n",
      "8.14542007446289\t5.510000228881836\t2.6354198455810547\n",
      "8.36269760131836\t9.14799976348877\t-0.7853021621704102\n",
      "8.199586868286133\t4.660999774932861\t3.5385870933532715\n",
      "9.255412101745605\t11.069999694824219\t-1.8145875930786133\n",
      "7.875412940979004\t6.164000034332275\t1.7114129066467285\n",
      "9.177287101745605\t6.7170000076293945\t2.460287094116211\n",
      "7.6551947593688965\t12.3100004196167\t-4.654805660247803\n",
      "9.29666805267334\t3.681999921798706\t5.614668130874634\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    n = 0\n",
    "    print(f'predicted\\tground truth\\tdiff')\n",
    "    for item in DataLoader(losses['ares val']['data'], batch_size=1, shuffle=True, num_workers=4):\n",
    "        item = item.to(device)\n",
    "        y_pred = model(x=item.x, edge_index=item.edge_index, edge_attr=item.edge_attr, batch=item.batch).view(-1)\n",
    "        print(f'{y_pred.cpu().item()}\\t{item.y.cpu().item()}\\t{y_pred.cpu().item()-item.y.cpu().item()}')\n",
    "        n += 1\n",
    "        if n == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Prev ares val 9.25\tBest ares val 9.25\n",
      "Prev ares test 12.33\tBest ares test 12.33\n",
      "Prev seg1 val 52.16\tBest seg1 val 52.16\n",
      "Prev seg1 test 55.66\tBest seg1 test 55.66\n",
      "Prev seg2 val 61.42\tBest seg2 val 61.42\n",
      "Prev seg2 test 63.43\tBest seg2 test 63.43\n",
      "Prev seg3 val 72.27\tBest seg3 val 72.27\n",
      "Prev seg3 test 74.27\tBest seg3 test 74.27\n"
     ]
    }
   ],
   "source": [
    "losses = {\n",
    "    'ares val': {\n",
    "        'data': ares_val_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'ares test': {\n",
    "        'data': ares_test_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg1 val': {\n",
    "        'data': seg1_val_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg1 test': {\n",
    "        'data': seg1_test_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg2 val': {\n",
    "        'data': seg2_val_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg2 test': {\n",
    "        'data': seg2_test_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg3 val': {\n",
    "        'data': seg3_val_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg3 test': {\n",
    "        'data': seg3_test_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "}\n",
    "\n",
    "for key in losses.keys():\n",
    "    process_epoch(0, batch_size, device, model, loss_fn, mse_no_reduce, optimizer, patience, losses, key, best_model, best_loss, save_values=True, plot=False)\n",
    "    values = losses[key]['values'].pop()\n",
    "    plt.boxplot([values['predicted'], values['true']], labels=['Predicted', 'Ground Truth'])\n",
    "    plt.title(f'{key.capitalize()} Values Box Plot (Transfer Learning)')\n",
    "    plt.ylabel('Values')\n",
    "    plt.savefig(os.path.join('values', f'{dataset}_{key.replace(\" \", \"_\")}.png'))\n",
    "    plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(list(losses.keys()), list(map(lambda l: l['losses'][0], losses.values())))\n",
    "plt.title(f'Average Loss Across Datasets (Transfer Learning)')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(os.path.join('loss', f'{dataset}.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "rects = plt.bar(list(losses.keys()), list(map(lambda l: l['losses'][0], losses.values())))\n",
    "plt.bar_label(rects, fmt='%0.0f', padding=3)\n",
    "plt.title(f'Average Loss Across Datasets (Transfer Learning)')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(os.path.join('loss', f'{dataset}.png'))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
