{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from time import time_ns\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from rnaquanet.network.graph_regression_network import GraphRegressionNetwork\n",
    "from rnaquanet.network.grn_data_module import GRNDataModule\n",
    "from rnaquanet.utils.rnaquanet_config import RnaquanetConfig\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "from torch.nn import (\n",
    "    BatchNorm1d,\n",
    "    Identity,\n",
    "    ReLU,\n",
    "    LeakyReLU,\n",
    "    Linear,\n",
    "    MSELoss\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    GATConv,\n",
    "    GCNConv,\n",
    "    Sequential,\n",
    "    global_mean_pool,\n",
    "    BatchNorm,\n",
    "    TransformerConv\n",
    ")\n",
    "from torch_geometric.nn.models import (\n",
    "    GAT\n",
    ")\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "from rnaquanet.data.preprocessing.hdf5_utils import load_data_from_hdf5\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from rnaquanet.network.h5_graph_dataset import H5GraphDataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg2_train_data = H5GraphDataset('/app/data/segments_2_coords/train.h5', max_iterations=80000).__enter__()\n",
    "\n",
    "seg1_val_data = H5GraphDataset('/app/data/segments_1_coords/val.h5').__enter__()\n",
    "seg1_test_data = H5GraphDataset('/app/data/segments_1_coords/test.h5').__enter__()\n",
    "\n",
    "seg2_val_data = H5GraphDataset('/app/data/segments_2_coords/val.h5').__enter__()\n",
    "seg2_test_data = H5GraphDataset('/app/data/segments_2_coords/test.h5').__enter__()\n",
    "\n",
    "seg3_val_data = H5GraphDataset('/app/data/segments_3_coords/val.h5').__enter__()\n",
    "seg3_test_data = H5GraphDataset('/app/data/segments_3_coords/test.h5').__enter__()\n",
    "\n",
    "ares_val_data = H5GraphDataset('/app/data/ares/val.h5').__enter__()\n",
    "ares_test_data = H5GraphDataset('/app/data/ares/test.h5').__enter__()\n",
    "\n",
    "dataset = 'seg2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_chart(losses, epoch):\n",
    "    plt.figure(figsize=(10,6))\n",
    "\n",
    "    bars = len(losses.values()) # num of bars\n",
    "    epochs = range(1, epoch+2)\n",
    "    width = 1.0/bars * 0.75\n",
    "    multiplier = 0\n",
    "    _, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "    for key, value in losses.items():\n",
    "        x = np.arange(len(value['losses']))\n",
    "\n",
    "        loss_values = value['losses']\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, loss_values, width, label=key, color=value['color'], alpha=0.5)\n",
    "        ax.bar_label(rects, fmt='%0.0f', padding=3)\n",
    "        ax.plot(x+((bars-1)/2)*width, loss_values, color=value['color'], alpha=0.5)\n",
    "        multiplier += 1\n",
    "\n",
    "    x = np.arange(len(epochs))\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_xticks(x + ((bars-1)/2)*width, epochs)\n",
    "    ax.legend()\n",
    "    plt.savefig(os.path.join('train', f'{dataset}.png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_desc(epoch, losses):\n",
    "    desc = f'Epoch {epoch+1}\\n'\n",
    "    for key, value in losses.items():\n",
    "        if value['losses']:\n",
    "            desc += f'Prev {key} {value[\"losses\"][-1]:.2f}\\tBest {key} {np.min(value[\"losses\"]):.2f}\\n'\n",
    "    return desc.rstrip('\\n')\n",
    "\n",
    "def process_epoch(\n",
    "        epoch, \n",
    "        batch_size, \n",
    "        device, \n",
    "        model,\n",
    "        loss_fn,\n",
    "        mse_no_reduce,\n",
    "        optimizer,\n",
    "        patience, \n",
    "        \n",
    "        losses, key,\n",
    "        best_model, best_loss,\n",
    "\n",
    "        save_values = False,\n",
    "        plot = True\n",
    "    ):\n",
    "    step = []\n",
    "    values = {'predicted': [], 'true': []}\n",
    "    assert(not losses[key]['train'] or not save_values)\n",
    "\n",
    "    if losses[key]['data'].max_iterations != None:\n",
    "        losses[key]['data'].shuffle()\n",
    "    \n",
    "\n",
    "    with tqdm(total=len(losses[key]['data']), desc=f'{key.capitalize()} epoch {epoch+1}', position=0, leave=True) as progressbar:\n",
    "        if not losses[key]['train']:\n",
    "            with torch.no_grad():\n",
    "                for item in DataLoader(losses[key]['data'], batch_size=batch_size, shuffle=False, num_workers=4):\n",
    "                    item = item.to(device)\n",
    "                    y_pred = model(x=item.x, edge_index=item.edge_index, edge_attr=item.edge_attr, batch=item.batch).view(-1)\n",
    "                    step.extend(mse_no_reduce(y_pred, item.y).cpu().tolist())\n",
    "                    if save_values:\n",
    "                        values['predicted'].extend(y_pred.cpu().tolist())\n",
    "                        values['true'].extend(item.y.view(-1).cpu().tolist())\n",
    "                    progressbar.update(item.y.cpu().shape[0])\n",
    "        else:\n",
    "            for item in DataLoader(losses[key]['data'], batch_size=batch_size, shuffle=(losses[key]['data'].max_iterations == None), num_workers=4):\n",
    "                item = item.to(device)\n",
    "                y_pred = model(x=item.x, edge_index=item.edge_index, edge_attr=item.edge_attr, batch=item.batch).view(-1)\n",
    "                loss = loss_fn(y_pred, item.y)\n",
    "                step.extend(mse_no_reduce(y_pred, item.y).cpu().tolist())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                progressbar.update(item.y.cpu().shape[0])\n",
    "        \n",
    "    losses[key]['losses'].append(np.mean(step))\n",
    "    if save_values:\n",
    "        losses[key]['values'].append(values)\n",
    "    clear_output(wait=True)\n",
    "    print(get_desc(epoch, losses))\n",
    "    if plot:\n",
    "        plot_loss_chart(losses, epoch)\n",
    "\n",
    "    index = np.argmin(losses[key]['losses'])\n",
    "    if losses[key]['patience']:\n",
    "        if losses[key]['losses'][-1] < best_loss:\n",
    "            return model.state_dict().copy(), losses[key]['losses'][index], False\n",
    "    return best_model, best_loss, losses[key]['patience'] and index <= (epoch-patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "torch.manual_seed(2137)\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "max_epochs = 1000\n",
    "batch_size = 32\n",
    "patience = 3\n",
    "\n",
    "model = Sequential('x, edge_index, edge_attr, batch', [\n",
    "    (TransformerConv(in_channels=99, out_channels=256, heads=4, dropout=0.5), 'x, edge_index -> x'),\n",
    "    (BatchNorm(in_channels=256*4), 'x -> x'),\n",
    "    (ReLU(), 'x -> x'),\n",
    "    (TransformerConv(in_channels=256*4, out_channels=256, heads=8, dropout=0.5), 'x, edge_index -> x'),\n",
    "    (BatchNorm(in_channels=256*8), 'x -> x'),\n",
    "    (ReLU(), 'x -> x'),\n",
    "\n",
    "    (GCNConv(in_channels=256*8, out_channels=256), 'x, edge_index -> x'),\n",
    "    (global_mean_pool, 'x, batch -> x'),\n",
    "\n",
    "    (Linear(in_features=256, out_features=64), 'x -> x'),\n",
    "    (ReLU(), 'x -> x'),\n",
    "    (Linear(in_features=64, out_features=1), 'x -> x'),\n",
    "]).to(device)\n",
    "\n",
    "loss_fn = MSELoss(reduction='mean').to(device)\n",
    "mse_no_reduce = MSELoss(reduction='none').to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "losses = {\n",
    "    'seg2 train': {\n",
    "        'data': seg2_train_data,\n",
    "        'losses': [],\n",
    "        'color': 'red',\n",
    "        'train': True,\n",
    "        'patience': False\n",
    "    },\n",
    "    'seg2 val': {\n",
    "        'data': seg2_val_data,\n",
    "        'losses': [],\n",
    "        'color': 'blue',\n",
    "        'train': False,\n",
    "        'patience': True,\n",
    "        'values': []\n",
    "    },\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    for key in losses.keys():\n",
    "        best_model, best_loss, cond = process_epoch(epoch, batch_size, device, model, loss_fn, mse_no_reduce, optimizer, patience, losses, key, best_model, best_loss)\n",
    "        if cond:\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "\n",
    "torch.save(best_model, os.path.join('model', f'{dataset}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted\tground truth\tdiff\n",
      "5.882852554321289\t10.946999549865723\t-5.064146995544434\n",
      "4.493436813354492\t12.600000381469727\t-8.106563568115234\n",
      "4.261335849761963\t2.6630001068115234\t1.5983357429504395\n",
      "7.084010601043701\t6.361999988555908\t0.722010612487793\n",
      "4.737633228302002\t3.562999963760376\t1.174633264541626\n",
      "4.923404693603516\t2.263000011444092\t2.660404682159424\n",
      "7.397254467010498\t7.710000038146973\t-0.3127455711364746\n",
      "5.132656097412109\t2.0810000896453857\t3.0516560077667236\n",
      "6.339267730712891\t6.539999961853027\t-0.20073223114013672\n",
      "4.24782657623291\t0.9160000085830688\t3.3318265676498413\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    n = 0\n",
    "    print(f'predicted\\tground truth\\tdiff')\n",
    "    for item in DataLoader(losses['seg2 val']['data'], batch_size=1, shuffle=True, num_workers=4):\n",
    "        item = item.to(device)\n",
    "        y_pred = model(x=item.x, edge_index=item.edge_index, edge_attr=item.edge_attr, batch=item.batch).view(-1)\n",
    "        print(f'{y_pred.cpu().item()}\\t{item.y.cpu().item()}\\t{y_pred.cpu().item()-item.y.cpu().item()}')\n",
    "        n += 1\n",
    "        if n == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Prev ares val 188.51\tBest ares val 188.51\n",
      "Prev ares test 129.78\tBest ares test 129.78\n",
      "Prev seg1 val 15.69\tBest seg1 val 15.69\n",
      "Prev seg1 test 17.47\tBest seg1 test 17.47\n",
      "Prev seg2 val 18.92\tBest seg2 val 18.92\n",
      "Prev seg2 test 18.60\tBest seg2 test 18.60\n",
      "Prev seg3 val 43.92\tBest seg3 val 43.92\n",
      "Prev seg3 test 48.61\tBest seg3 test 48.61\n"
     ]
    }
   ],
   "source": [
    "losses = {\n",
    "    'ares val': {\n",
    "        'data': ares_val_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'ares test': {\n",
    "        'data': ares_test_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg1 val': {\n",
    "        'data': seg1_val_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg1 test': {\n",
    "        'data': seg1_test_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg2 val': {\n",
    "        'data': seg2_val_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg2 test': {\n",
    "        'data': seg2_test_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg3 val': {\n",
    "        'data': seg3_val_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "    'seg3 test': {\n",
    "        'data': seg3_test_data,\n",
    "        'losses': [],\n",
    "        'color': '',\n",
    "        'train': False,\n",
    "        'patience': False,\n",
    "        'values': []\n",
    "    },\n",
    "}\n",
    "\n",
    "for key in losses.keys():\n",
    "    process_epoch(0, batch_size, device, model, loss_fn, mse_no_reduce, optimizer, patience, losses, key, best_model, best_loss, save_values=True, plot=False)\n",
    "    values = losses[key]['values'].pop()\n",
    "    plt.boxplot([values['predicted'], values['true']], labels=['Predicted', 'Ground Truth'])\n",
    "    plt.title(f'{key.capitalize()} Values Box Plot (Trained on Seg2)')\n",
    "    plt.ylabel('Values')\n",
    "    plt.savefig(os.path.join('values', f'{dataset}_{key.replace(\" \", \"_\")}.png'))\n",
    "    plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(list(losses.keys()), list(map(lambda l: l['losses'][0], losses.values())))\n",
    "plt.title(f'Average Loss Across Datasets (Trained on Seg2)')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(os.path.join('loss', f'{dataset}.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "rects = plt.bar(list(losses.keys()), list(map(lambda l: l['losses'][0], losses.values())))\n",
    "plt.bar_label(rects, fmt='%0.0f', padding=3)\n",
    "plt.title(f'Average Loss Across Datasets (Trained on Seg2)')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(os.path.join('loss', f'{dataset}.png'))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
